{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting heart rate during excercise with LSTM\n",
    "\n",
    "Training an LSTM model to predict heart rate in n seconds in future based on the sensor measurements over the past 60 seconds. The data is collected from Garmin Fenix 6s during running excercises, performed by the author, mostly in a hilly environment. The fit files data was converted into csv files with fitdecode library https://github.com/polyvertex/fitdecode. Fit-files format is used at least by Garmin and Suunto devices.\n",
    "\n",
    "Some of the useful inputs available for cycling or running include:\n",
    "- heart rate\n",
    "- cadence\n",
    "- speed\n",
    "- altitude\n",
    "- grade \n",
    "- power\n",
    "- distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load libraries\n",
    "import os, glob \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# location of the fit files\n",
    "fit_path = \"/home/riku/projects/athlet_hr_predict/fit_file_csv\"\n",
    "fit_test_path = \"/home/riku/projects/athlet_hr_predict/fit_file_test_csv\"\n",
    "graph_path = \"/home/riku/projects/athlet_hr_predict/graphs\"\n",
    "os.chdir(fit_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add calculated altitude difference column, and 5sec moving average column. Remove geographical coordinates for privacy.\n",
    "fit_files = glob.glob(\"*.csv\")\n",
    "for file in fit_files:\n",
    "    df = pd.read_csv(fit_path+'/'+file, index_col='timestamp')\n",
    "    df['alt_difference'] = df['enhanced_altitude'] - df['enhanced_altitude'].shift(1)\n",
    "    df['rolling_ave_alt'] = df['alt_difference'].rolling(window=5).mean()\n",
    "    df = df.bfill()\n",
    "    df = df.drop(['position_lat','position_long'], axis=1, errors='ignore')\n",
    "    df.to_csv(fit_path+'/'+file)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set parameters, file names\n",
    "\n",
    "#select the features for EDA graphs:\n",
    "eda_model_features =  [\"heart_rate\", \"enhanced_speed\",\"rolling_ave_alt\", \"cadence\", \"distance\", \"enhanced_altitude\"] #  cadence, enhanced_altitude, distance, heart_rate, enhanced_speed, rolling_ave_alt\n",
    "\n",
    "#select the predictors for the model:\n",
    "model_features =  [\"heart_rate\", \"enhanced_speed\",\"rolling_ave_alt\",\"cadence\"] #  cadence, altitude, distance, heart_rate, enhanced_speed, rolling_ave_alt\n",
    "batch_size = 500 # training batch size for the LSTM\n",
    "epochs = 180 # maximum number of epochs - autostop will work on per file basis\n",
    "learning_rate = 0.002\n",
    "n_X = 120 # number of timesteps for training\n",
    "n_y = 20 # number of timesteps in future for prediction\n",
    "step = 1 # step size of predictors for model training\n",
    "\n",
    "sequence_length = int(n_X/step)\n",
    "n_fit_files_test_set = 10 # number of files for validation dataset (only 1 validation file supported at the moment)\n",
    "\n",
    "# select the training files and the validation files \n",
    "train_files = glob.glob(fit_path+\"/*.csv\")[0:-n_fit_files_test_set]\n",
    "valid_files = glob.glob(fit_path+\"/*.csv\")[-n_fit_files_test_set:]\n",
    "test_files = glob.glob(fit_test_path+\"/*.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate the data normalisation parameters from all training data\n",
    "\n",
    "def normalize(data):\n",
    "    data_mean = data.mean(axis=0)\n",
    "    data_std = data.std(axis=0)\n",
    "    #return (data - data_mean) / data_std, data_mean, data_std\n",
    "    return data_mean, data_std\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "df_mean, df_std = normalize(df)\n",
    "\n",
    "def denormalize_hr(data):\n",
    "    return data*df_std[0]+df_mean[0] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Short EDA\n",
    "\n",
    "Selected features are shown on the plots. The fit file here was collected during a hilly 5km run. It shows large variability during the workout in heart rate, speed and altitude. Cadence is relatively constant throughout the excercise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eda_example = pd.read_csv(fit_path+'/RUN_2021-08-26-07-15-23.fit.csv', index_col='timestamp')[eda_model_features]\n",
    "\n",
    "def show_raw_visualization(data):\n",
    "    time_data = data.index\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=int(len(eda_model_features)/2+0.5), ncols=2, figsize=(9, 5), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(eda_model_features)):\n",
    "        key = eda_model_features[i]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color='b',\n",
    "            title=\"{}\".format(key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "    plt.tight_layout()\n",
    "\n",
    "show_raw_visualization(eda_example)\n",
    "plt.savefig(graph_path+\"/HR_eda.png\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create validation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# validation dataset - train each file at the time\n",
    "n=0\n",
    "for file in valid_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]]\n",
    "    \n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    if n==0 : dataset_val_old = dataset_val\n",
    "    if n>0 : dataset_val_old = dataset_val.concatenate(dataset_val_old)\n",
    "    \n",
    "    n=n+1\n",
    "\n",
    "dataset_val = dataset_val_old"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# calculate stats for a naive model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate statistics for the naive model\n",
    "# make dataframe for the naive model\n",
    "d_naive = pd.DataFrame(columns=['measured', 'predicted'])\n",
    "d_naive['measured']=denormalize_hr(x[n_y:,0])\n",
    "d_naive['predicted']=denormalize_hr(x[:-n_y,0])\n",
    "\n",
    "# calculate some stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy\n",
    "\n",
    "y_test, pred_test = d_naive['measured'].values,d_naive['predicted'].values\n",
    "\n",
    "MSE_test=round(mean_squared_error(y_test, pred_test, squared=True),3)\n",
    "MAE_test=round(mean_absolute_error(y_test, pred_test),3)\n",
    "\n",
    "test_sdev = np.std(pred_test-y_test)*1.96\n",
    "test_mean = np.mean(pred_test-y_test)\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h, h\n",
    "\n",
    "mean_s, ci95_l, ci95_h, mean_uncertainty = mean_confidence_interval(data=(pred_test-y_test))\n",
    "\n",
    "print('Naive model\\nMAE = '+ str(MAE_test)+\", MSE = \"+str(MSE_test))\n",
    "print ('Mean and 95% prediction interval = {} +/- {}'.format(test_mean,test_sdev))\n",
    "print('Uncertainty of mean = '+ str(mean_uncertainty))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the shapes of X & y for a batch\n",
    "for batch in dataset_val.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "# the model\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(4, return_sequences=True)(inputs)\n",
    "#lstm_out = keras.layers.Conv1D(2, kernel_size=5)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mae\", \"acc\"], loss=\"mae\")\n",
    "model.summary()\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, verbose=1)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the training data, train the model file by file\n",
    "\n",
    "Each file is processed separately, and training epochs repeated as long as the model improves for each model up to the variable $epochs$ defined earlier. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training dataset\n",
    "\n",
    "n=0\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    print(file)\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]].values\n",
    "    \n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    if n==0 : dataset_train_old = dataset_train\n",
    "    if n>0 : dataset_train_old = dataset_train.concatenate(dataset_train_old)\n",
    "\n",
    "    n=n+1\n",
    "\n",
    "dataset_train=dataset_train_old\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    "    verbose=0\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "plt.savefig(graph_path+'/HR_his_t'+str(n_y)+\".png\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check the model predictions visually"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n=0\n",
    "for file in test_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    print(file)\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]].values\n",
    "    \n",
    "    dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=10\n",
    "    )\n",
    "    \n",
    "    if n>0:\n",
    "        dataset_test_old = dataset_train_old.concatenate(dataset_test)\n",
    "    \n",
    "    dataset_test_old = dataset_test\n",
    "    \n",
    "    n=n+1\n",
    "dataset_test = dataset_test_old"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.ylim(100,170)\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_test.take(5):\n",
    "    show_plot(\n",
    "        [denormalize_hr(x[0][:, 0].numpy()), denormalize_hr(y[0]), denormalize_hr( model.predict(x)[0])],\n",
    "        n_y,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation \n",
    "\n",
    "under construction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a testing dataset from the kept-aside files\n",
    "n=0\n",
    "for file in test_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]].values\n",
    "    \n",
    "    dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    if n==0 : dataset_test_old = dataset_test\n",
    "    if n>0 : dataset_test_old = dataset_test_old.concatenate(dataset_test)\n",
    "    \n",
    "    n=n+1\n",
    "dataset_test = dataset_test_old\n",
    "\n",
    "# make a dataframe with predictions and observations\n",
    "d=pd.DataFrame([0,1])\n",
    "for x, y in dataset_test:\n",
    "    a = denormalize_hr(y[0]).numpy(), denormalize_hr( model.predict(x)[0])\n",
    "    c = pd.DataFrame(a).T\n",
    "    d = d.merge(c, how='outer')\n",
    "d.columns=['measured', 'predicted']\n",
    "d = d.bfill()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# time domain plot with observed blue, and predicted orange. Predicted is calculated values where previous 30sec of inputs are missing.\n",
    "fig, ax1 = plt.subplots(1,1)\n",
    "fig.set_size_inches(9, 2.3)\n",
    "d.plot(ylim=(100,180), xlabel='Timestep', ylabel='HR', ax=ax1)\n",
    "plt.savefig(graph_path+'/HR_ex_t'+str(n_y)+\".png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate some stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy\n",
    "\n",
    "y_test, pred_test = d['measured'].values,d['predicted'].values\n",
    "\n",
    "MSE_test=round(mean_squared_error(y_test, pred_test, squared=True),3)\n",
    "MAE_test=round(mean_absolute_error(y_test, pred_test),3)\n",
    "\n",
    "test_sdev = np.std(pred_test-y_test)*1.96\n",
    "test_mean = np.mean(pred_test-y_test)\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h, h\n",
    "\n",
    "mean_s, ci95_l, ci95_h, mean_uncertainty = mean_confidence_interval(data=(pred_test-y_test))\n",
    "\n",
    "print('Test dataset\\nMAE = '+ str(MAE_test)+\", MSE = \"+str(MSE_test))\n",
    "print ('Mean and 95% prediction interval = {} +/- {}'.format(test_mean,test_sdev))\n",
    "print('Uncertainty of mean = '+ str(mean_uncertainty))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# graph the model performance\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "m, b = np.polyfit(y_test, pred_test, 1)\n",
    "\n",
    "fig, ((ax1, ax2, ax3)) = plt.subplots(1,3)\n",
    "fig.set_size_inches(9, 2.3)\n",
    "sns.kdeplot( x=pred_test-y_test, fill=True, ax=ax1, common_norm=False)\n",
    "ax2.scatter(x=y_test, y=pred_test, s=1, alpha=0.4)\n",
    "ax2.plot(y_test, m*y_test + b, c='black')\n",
    "sm.qqplot((pred_test-y_test), line ='45', ax=ax3)\n",
    "\n",
    "ax1.set_xlim(-20,20)\n",
    "ax1.set_xlabel('Error')\n",
    "ax2.set_xlabel('Observed HR')\n",
    "ax2.set_ylabel('Predicted HR')\n",
    "ax2.set_xlim(100,180)\n",
    "ax2.set_ylim(100,180)\n",
    "ax3.set_xlim(-15,15)\n",
    "ax3.set_ylim(-15,15)\n",
    "ax1.text(-19.8,0.0,'LSTM test dataset\\nt='+ str(n_y) + '\\nMAE='+str(MAE_test) + \"\\nmean=\"+ str(round(mean_s,5)) + \"\\nUm= ±\"+ str(round(mean_uncertainty,4))+'\\np95= ±'+ str(round(test_sdev,2)) , fontsize=8 )\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "ax3.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(graph_path+'/HR_t'+str(n_y)+'-'+str(n_X) +\".png\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# further residuals plots\n",
    "fig, ((ax1, ax2)) = plt.subplots(1,2)\n",
    "fig.set_size_inches(9, 2)\n",
    "\n",
    "y_pred_error = (pred_test - y_test)\n",
    "x_n = np.arange(0,len(y_pred_error))\n",
    "\n",
    "sns.regplot(x=pred_test, y=y_pred_error, scatter=False,  ax=ax1, ci=95, lowess=True)\n",
    "sns.regplot(x=x_n, y=y_pred_error, scatter=False,  ax=ax2, ci=95, lowess=True)\n",
    "sns.scatterplot(x=pred_test, y=y_pred_error,  ax=ax1, alpha = 0.7, s=4)\n",
    "sns.scatterplot(x=x_n, y=y_pred_error,  ax=ax2, alpha = 0.7, s=4)\n",
    "\n",
    "ax2.set_ylim(-20,20)\n",
    "ax1.set_ylim(-20,20)\n",
    "ax2.set_xlabel('Timestep')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax1.set_xlabel('Predicted CQI')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(graph_path+'/HR_res_t'+str(n_y)+\".png\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('hrpredict': conda)"
  },
  "interpreter": {
   "hash": "b77292dc63b48aca88a1593f58a471620914a6081d352eacb6540144f7e5fab5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}